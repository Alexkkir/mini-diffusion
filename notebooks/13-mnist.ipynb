{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akkirr/annotated-diffusion\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nan_to_num\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim import Adam\n",
    "\n",
    "from copy import deepcopy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylib import *\n",
    "import mylora\n",
    "import lora_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(linear_beta_schedule, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"results_folder\": \"PosixPath('results-mnist/1-baseline')\",\n",
       "    \"image_size\": 28,\n",
       "    \"channels\": 1,\n",
       "    \"batch_size\": 128,\n",
       "    \"device\": \"cuda\",\n",
       "    \"checkpoint\": \"checkpoints/3-mnist-0:1,3:9.pt\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = Settings(\n",
    "    results_folder = Path(\"./results-mnist/1-baseline\"),\n",
    "    image_size = 28,\n",
    "    channels = 1,\n",
    "    batch_size = 128,\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    checkpoint = 'checkpoints/3-mnist-0:1,3:9.pt'\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.results_folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mnist (/home/akkirr/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e23fb1628c48e58801650731dd99b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/akkirr/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332/cache-d18d5dcedca7f7f9.arrow\n",
      "Loading cached processed dataset at /home/akkirr/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332/cache-01fe572201716064.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"mnist\")\n",
    "# define image transformations (e.g. using torchvision)\n",
    "transform = Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "# define function\n",
    "def transforms(examples):\n",
    "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
    "   del examples[\"image\"]\n",
    "\n",
    "   return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms).filter(lambda x: x['label'] != 2).remove_columns(\"label\")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=settings.batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable layers:           231\n",
      "frozen layers:                0\n",
      "total params:           2020257\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds()\n",
    "model = Unet(\n",
    "    dim=settings.image_size,\n",
    "    channels=settings.channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ")\n",
    "\n",
    "model.to(settings.device)\n",
    "mylora.model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c684290dd248ceb3a8e223196cd8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.38870003819465637\n",
      "Loss: 0.040315043181180954\n",
      "Loss: 0.02934751659631729\n",
      "Loss: 0.028124278411269188\n",
      "Loss: 0.02748536504805088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64cb0a797d549ddad9ebd225beb77e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.025776633992791176\n",
      "Loss: 0.022529777139425278\n",
      "Loss: 0.022077929228544235\n",
      "Loss: 0.022946104407310486\n",
      "Loss: 0.023017099127173424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7befb1655a4779933c5aaa96db4d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02249987982213497\n",
      "Loss: 0.020714594051241875\n",
      "Loss: 0.020471815019845963\n",
      "Loss: 0.021730223670601845\n",
      "Loss: 0.020951353013515472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d91873d00446cc9fba062711483396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02081342041492462\n",
      "Loss: 0.019031308591365814\n",
      "Loss: 0.01930704154074192\n",
      "Loss: 0.020853281021118164\n",
      "Loss: 0.0201482642441988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe322ed7d00e4b4183c3f2395d43d198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02025529555976391\n",
      "Loss: 0.0182176623493433\n",
      "Loss: 0.018395226448774338\n",
      "Loss: 0.0201185904443264\n",
      "Loss: 0.019668594002723694\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c222fa782fa9446dac38d1997eff76e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.019672637805342674\n",
      "Loss: 0.01785200461745262\n",
      "Loss: 0.018006399273872375\n",
      "Loss: 0.019611923024058342\n",
      "Loss: 0.018802886828780174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a004085825c46be99c386a186c1a8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.019213758409023285\n",
      "Loss: 0.01745474711060524\n",
      "Loss: 0.017739497125148773\n",
      "Loss: 0.019340617582201958\n",
      "Loss: 0.01843217946588993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46200ec1b897425a9a3b0d7c2d0c6263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.01878400892019272\n",
      "Loss: 0.017494192346930504\n",
      "Loss: 0.01738337241113186\n",
      "Loss: 0.019208043813705444\n",
      "Loss: 0.018164334818720818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3b5ddaad0142a8a35db73ef36a4e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.018438026309013367\n",
      "Loss: 0.017084669321775436\n",
      "Loss: 0.01701967976987362\n",
      "Loss: 0.018673282116651535\n",
      "Loss: 0.01828204281628132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8fb0dafcc44a7f8ee967b5883f2d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.018175700679421425\n",
      "Loss: 0.016921713948249817\n",
      "Loss: 0.01689371094107628\n",
      "Loss: 0.018432723358273506\n",
      "Loss: 0.017681581899523735\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500d6130d8c14c83af460b7ecc164659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "train(model, optimizer, dataloader, sampler, settings, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"results_folder\": \"PosixPath('results-mnist/1-baseline')\",\n",
       "    \"image_size\": 28,\n",
       "    \"channels\": 1,\n",
       "    \"batch_size\": 128,\n",
       "    \"device\": \"cuda\",\n",
       "    \"checkpoint\": \"checkpoints/3-mnist-0:1,3:9.pt\"\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/opt/conda/conda-bld/ffmpeg_1597178665428/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh --cc=/opt/conda/conda-bld/ffmpeg_1597178665428/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "Input #0, image2, from 'results-mnist/1-baseline/sample-%d.png':\n",
      "  Duration: 00:00:01.57, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 242x242, 7 fps, 7 tbr, 7 tbn, 7 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> gif (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, gif, to 'results-mnist/1-baseline/sample.gif':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0: Video: gif, bgr8, 242x242, q=2-31, 200 kb/s, 7 fps, 100 tbn, 7 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.91.100 gif\n",
      "frame=   11 fps=0.0 q=-0.0 Lsize=      86kB time=00:00:01.44 bitrate= 488.4kbits/s speed=25.6x    \n",
      "video:86kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.022756%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"ffmpeg -f image2 -framerate 7 -i {str(settings.results_folder)}/sample-%d.png -loop -0 {str(settings.results_folder)}/sample.gif -y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), settings.checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train lora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injected lora    28 x 2 x 384   in downs.0.2.fn.fn.to_qkv\n",
      "Injected lora   128 x 2 x 28    in downs.0.2.fn.fn.0\n",
      "Injected lora    28 x 2 x 384   in downs.1.2.fn.fn.to_qkv\n",
      "Injected lora   128 x 2 x 28    in downs.1.2.fn.fn.0\n",
      "Injected lora    56 x 2 x 384   in downs.2.2.fn.fn.to_qkv\n",
      "Injected lora   128 x 2 x 56    in downs.2.2.fn.fn.0\n",
      "Injected lora   112 x 2 x 384   in ups.0.2.fn.fn.to_qkv\n",
      "Injected lora   128 x 2 x 112   in ups.0.2.fn.fn.0\n",
      "Injected lora    56 x 2 x 384   in ups.1.2.fn.fn.to_qkv\n",
      "Injected lora   128 x 2 x 56    in ups.1.2.fn.fn.0\n",
      "Injected lora    28 x 2 x 384   in ups.2.2.fn.fn.to_qkv\n",
      "Injected lora   128 x 2 x 28    in ups.2.2.fn.fn.0\n",
      "\n",
      "trainable layers:            24\n",
      "frozen layers:              231\n",
      "total params:           2027633\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds()\n",
    "model = Unet(\n",
    "    dim=settings.image_size,\n",
    "    channels=settings.channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ")\n",
    "model.load_state_dict(torch.load(settings.checkpoint))\n",
    "\n",
    "mylora.inject_lora(\n",
    "    model, 2, 0.4,\n",
    "    ['LinearAttention'],\n",
    "    [nn.Conv2d]\n",
    ")\n",
    "model.to(settings.device)\n",
    "\n",
    "mylora.freeze_lora(model)\n",
    "print()\n",
    "mylora.model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/akkirr/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332/cache-4fb5f69a4c536345.arrow\n",
      "Loading cached processed dataset at /home/akkirr/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332/cache-cc87227bbbbe9a2e.arrow\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = dataset.with_transform(transforms).filter(lambda x: x['label'] == 2).remove_columns(\"label\")\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=settings.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915b13186e4342cd8d357dcd67499740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m settings\u001b[39m.\u001b[39mresults_folder\u001b[39m.\u001b[39mmkdir(exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m train(model, optimizer, dataloader, sampler, settings, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/train.py:24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader, sampler, settings, epochs, visualize_type, save_first_sample)\u001b[0m\n\u001b[1;32m     22\u001b[0m visualize \u001b[39m=\u001b[39m visualize_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m visualize \u001b[39mand\u001b[39;00m save_first_sample:\n\u001b[0;32m---> 24\u001b[0m     save_images(model, sampler, settings, visualize_type, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/train.py:75\u001b[0m, in \u001b[0;36msave_images\u001b[0;34m(model, sampler, settings, visialize_type, milestone)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39melif\u001b[39;00m visialize_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgrid\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     batches \u001b[39m=\u001b[39m num_to_groups(\u001b[39m64\u001b[39m, batch_size)\n\u001b[0;32m---> 75\u001b[0m     all_images_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m     76\u001b[0m         \u001b[39mmap\u001b[39;49m(\n\u001b[1;32m     77\u001b[0m             \u001b[39mlambda\u001b[39;49;00m n: sampler\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m     78\u001b[0m                 model,\n\u001b[1;32m     79\u001b[0m                 image_size\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mimage_size,\n\u001b[1;32m     80\u001b[0m                 batch_size\u001b[39m=\u001b[39;49mn,\n\u001b[1;32m     81\u001b[0m                 channels\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mchannels,\n\u001b[1;32m     82\u001b[0m             ),\n\u001b[1;32m     83\u001b[0m             batches,\n\u001b[1;32m     84\u001b[0m         )\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     all_images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(all_images_list[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     87\u001b[0m     all_images \u001b[39m=\u001b[39m (all_images \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/train.py:77\u001b[0m, in \u001b[0;36msave_images.<locals>.<lambda>\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39melif\u001b[39;00m visialize_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgrid\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     batches \u001b[39m=\u001b[39m num_to_groups(\u001b[39m64\u001b[39m, batch_size)\n\u001b[1;32m     75\u001b[0m     all_images_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m     76\u001b[0m         \u001b[39mmap\u001b[39m(\n\u001b[0;32m---> 77\u001b[0m             \u001b[39mlambda\u001b[39;00m n: sampler\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m     78\u001b[0m                 model,\n\u001b[1;32m     79\u001b[0m                 image_size\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mimage_size,\n\u001b[1;32m     80\u001b[0m                 batch_size\u001b[39m=\u001b[39;49mn,\n\u001b[1;32m     81\u001b[0m                 channels\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mchannels,\n\u001b[1;32m     82\u001b[0m             ),\n\u001b[1;32m     83\u001b[0m             batches,\n\u001b[1;32m     84\u001b[0m         )\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     all_images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(all_images_list[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     87\u001b[0m     all_images \u001b[39m=\u001b[39m (all_images \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/diffusion.py:118\u001b[0m, in \u001b[0;36mSampler.sample\u001b[0;34m(self, model, image_size, batch_size, channels)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\u001b[39mself\u001b[39m, model, image_size, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_loop(\n\u001b[1;32m    119\u001b[0m         model, shape\u001b[39m=\u001b[39;49m(batch_size, channels, image_size, image_size)\n\u001b[1;32m    120\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/diffusion.py:110\u001b[0m, in \u001b[0;36mSampler.p_sample_loop\u001b[0;34m(self, model, shape)\u001b[0m\n\u001b[1;32m    103\u001b[0m imgs \u001b[39m=\u001b[39m []\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\n\u001b[1;32m    106\u001b[0m     \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimesteps)),\n\u001b[1;32m    107\u001b[0m     desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msampling loop time step\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     total\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimesteps,\n\u001b[1;32m    109\u001b[0m ):\n\u001b[0;32m--> 110\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample(\n\u001b[1;32m    111\u001b[0m         model, img, torch\u001b[39m.\u001b[39;49mfull((b,), i, device\u001b[39m=\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong), i\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m     imgs\u001b[39m.\u001b[39mappend(img\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m imgs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/diffusion.py:84\u001b[0m, in \u001b[0;36mSampler.p_sample\u001b[0;34m(self, model, x, t, t_index)\u001b[0m\n\u001b[1;32m     79\u001b[0m sqrt_recip_alphas_t \u001b[39m=\u001b[39m extract(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqrt_recip_alphas, t, x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     81\u001b[0m \u001b[39m# Equation 11 in the paper\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Use our model (noise predictor) to predict the mean\u001b[39;00m\n\u001b[1;32m     83\u001b[0m model_mean \u001b[39m=\u001b[39m sqrt_recip_alphas_t \u001b[39m*\u001b[39m (\n\u001b[0;32m---> 84\u001b[0m     x \u001b[39m-\u001b[39m betas_t \u001b[39m*\u001b[39m model(x, t) \u001b[39m/\u001b[39m sqrt_one_minus_alphas_cumprod_t\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m t_index \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m model_mean\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/model.py:300\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, time, x_self_cond)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mfor\u001b[39;00m block1, block2, attn, upsample \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mups:\n\u001b[1;32m    299\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x, h\u001b[39m.\u001b[39mpop()), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 300\u001b[0m     x \u001b[39m=\u001b[39m block1(x, t)\n\u001b[1;32m    302\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x, h\u001b[39m.\u001b[39mpop()), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    303\u001b[0m     x \u001b[39m=\u001b[39m block2(x, t)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/model.py:117\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[0;34m(self, x, time_emb)\u001b[0m\n\u001b[1;32m    114\u001b[0m     time_emb \u001b[39m=\u001b[39m rearrange(time_emb, \u001b[39m\"\u001b[39m\u001b[39mb c -> b c 1 1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m     scale_shift \u001b[39m=\u001b[39m time_emb\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock1(x, scale_shift\u001b[39m=\u001b[39;49mscale_shift)\n\u001b[1;32m    118\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock2(h)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m h \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres_conv(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/model.py:84\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, scale_shift)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, scale_shift\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 84\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(x)\n\u001b[1;32m     85\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m exists(scale_shift):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/annotated-diffusion/mylib/model.py:62\u001b[0m, in \u001b[0;36mWeightStandardizedConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\n\u001b[1;32m     61\u001b[0m mean \u001b[39m=\u001b[39m reduce(weight, \u001b[39m\"\u001b[39m\u001b[39mo ... -> o 1 1 1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m var \u001b[39m=\u001b[39m reduce(weight, \u001b[39m\"\u001b[39;49m\u001b[39mo ... -> o 1 1 1\u001b[39;49m\u001b[39m\"\u001b[39;49m, partial(torch\u001b[39m.\u001b[39;49mvar, unbiased\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[1;32m     63\u001b[0m normalized_weight \u001b[39m=\u001b[39m (weight \u001b[39m-\u001b[39m mean) \u001b[39m*\u001b[39m (var \u001b[39m+\u001b[39m eps)\u001b[39m.\u001b[39mrsqrt()\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[1;32m     66\u001b[0m     x,\n\u001b[1;32m     67\u001b[0m     normalized_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m     73\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/einops/einops.py:411\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     hashable_axes_lengths \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m(axes_lengths\u001b[39m.\u001b[39mitems()))\n\u001b[0;32m--> 411\u001b[0m     recipe \u001b[39m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[39m=\u001b[39;49mhashable_axes_lengths)\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_recipe(recipe, tensor, reduction_type\u001b[39m=\u001b[39mreduction)\n\u001b[1;32m    413\u001b[0m \u001b[39mexcept\u001b[39;00m EinopsError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/einops/einops.py:252\u001b[0m, in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_lengths)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Perform initial parsing of pattern and provided supplementary info\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39maxes_lengths is a tuple of tuples (axis_name, axis_length)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    251\u001b[0m left_str, rght_str \u001b[39m=\u001b[39m pattern\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m->\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m left \u001b[39m=\u001b[39m ParsedExpression(left_str)\n\u001b[1;32m    253\u001b[0m rght \u001b[39m=\u001b[39m ParsedExpression(rght_str)\n\u001b[1;32m    255\u001b[0m \u001b[39m# checking that axes are in agreement - new axes appear only in repeat, while disappear only in reduction\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/einops/parsing.py:93\u001b[0m, in \u001b[0;36mParsedExpression.__init__\u001b[0;34m(self, expression, allow_underscore, allow_duplicates)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[39mraise\u001b[39;00m EinopsError(\u001b[39m\"\u001b[39m\u001b[39mAxis composition is one-level (brackets inside brackets not allowed)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m     bracket_group \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 93\u001b[0m \u001b[39melif\u001b[39;00m char \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m bracket_group \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m         \u001b[39mraise\u001b[39;00m EinopsError(\u001b[39m'\u001b[39m\u001b[39mBrackets are not balanced\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "settings.results_folder = Path(\"./results-mnist/2-rank=2_do=0.25\")\n",
    "settings.results_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "train(model, optimizer, dataloader, sampler, settings, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"ffmpeg -f image2 -framerate 7 -i {str(settings.results_folder)}/sample-%d.png -loop -0 {str(settings.results_folder)}/sample.gif -y\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train open-source lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable layers:            24\n",
      "frozen layers:              231\n",
      "total params:           2023945\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds()\n",
    "model = Unet(\n",
    "    dim=settings.image_size,\n",
    "    channels=settings.channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ")\n",
    "model.load_state_dict(torch.load(settings.checkpoint))\n",
    "\n",
    "model.requires_grad_(False)\n",
    "unet_lora_params, train_names = lora_diffusion.inject_trainable_lora_extended(\n",
    "    model,\n",
    "    target_replace_module=['LinearAttention'],\n",
    "    r=1,\n",
    ")\n",
    "model.to(settings.device)\n",
    "\n",
    "mylora.model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/akkirr/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332/cache-4fb5f69a4c536345.arrow\n",
      "Loading cached processed dataset at /home/akkirr/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332/cache-cc87227bbbbe9a2e.arrow\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = dataset.with_transform(transforms).filter(lambda x: x['label'] == 2).remove_columns(\"label\")\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=settings.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e00babce9cd430aa6ece4f720799746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02309069223701954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011813fd08fb4339a2f5ae2da8864d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.021512994542717934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17b377b24e14a40aa69a3038cb7d363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02117716521024704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e0743b8a5d4c699d0ac39988a971dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.021008169278502464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2464192176854ef699b300bb533dcff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.020907269790768623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae682e2c94f7459a8bba7a24d7371451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.020840400829911232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc4b2b224bf41a8a82cbe3b3754abcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.020795632153749466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593a7863c815442687b23972e1b6e54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02075948938727379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b1827abb5240fdab9e4284ee054400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02072523534297943\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02b6b7d48c14b50b299c52b928ae3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.020693954080343246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fac7c80eb2848088871f36b3a83e3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "settings.results_folder = Path(\"./results-mnist/3-os-lora-default\")\n",
    "settings.results_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "train(model, optimizer, dataloader, sampler, settings, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gifski created /home/akkirr/annotated-diffusion/results-mnist/3-os-lora-default/sample.gif"
     ]
    }
   ],
   "source": [
    "folder = str(settings.results_folder)\n",
    "! /home/akkirr/.cargo/bin/gifski -o $folder/sample.gif -r 7 $folder/sample-*.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
