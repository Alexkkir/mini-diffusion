{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akkirr/annotated-diffusion\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mylora\n",
    "import torch\n",
    "import mylib\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.QKV = nn.Linear(1, 1)\n",
    "        self.C = nn.Linear(1, 1)\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.C(self.lrelu(self.QKV(x)))\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_proj = nn.Linear(1, 1)\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lrelu(self.time_proj(x))\n",
    "\n",
    "\n",
    "class A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.just_linear = nn.Linear(1, 1)\n",
    "        self.attn = Attention()\n",
    "        self.time_embedder = TimeEmbedding()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attn(self.just_linear(x) + self.time_embedder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injected lora (1x2x1) in attn.QKV\n",
      "Injected lora (1x2x1) in attn.C\n"
     ]
    }
   ],
   "source": [
    "model = A()\n",
    "mylora.inject_lora(\n",
    "    model, 2, 0, [\"Attention\"], [nn.Linear], [mylora.LoraInjectedLinear], verbose=True\n",
    ")\n",
    "mylora.freeze_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attn.QKV',\n",
       "  LoraInjectedLinear(\n",
       "    (src_linear): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (lora_down): Linear(in_features=1, out_features=2, bias=False)\n",
       "    (lora_up): Linear(in_features=2, out_features=1, bias=False)\n",
       "    (dropout_layer): Dropout1d(p=0, inplace=False)\n",
       "  )),\n",
       " ('attn.C',\n",
       "  LoraInjectedLinear(\n",
       "    (src_linear): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (lora_down): Linear(in_features=1, out_features=2, bias=False)\n",
       "    (lora_up): Linear(in_features=2, out_features=1, bias=False)\n",
       "    (dropout_layer): Dropout1d(p=0, inplace=False)\n",
       "  ))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mylora.get_lora_modules(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attn.QKV.lora_down.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.6649],\n",
       "          [ 0.8149]])),\n",
       " ('attn.QKV.lora_up.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0.]])),\n",
       " ('attn.C.lora_down.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.5499],\n",
       "          [-0.2450]])),\n",
       " ('attn.C.lora_up.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0.]]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mylora.get_lora_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "p = model.get_parameter('attn.QKV.lora_up.weight')\n",
    "p.requires_grad = True\n",
    "print(model.get_parameter('attn.QKV.lora_up.weight').requires_grad)\n",
    "p.requires_grad = False\n",
    "print(model.get_parameter('attn.QKV.lora_up.weight').requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injected lora (1x2x1) in attn.QKV\n",
      "Injected lora (1x2x1) in attn.C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2017],\n",
       "        [0.4190]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylib.set_all_seeds(0)\n",
    "model = A()\n",
    "mylora.inject_lora(\n",
    "    model, 2, 0, [\"Attention\"], [nn.Linear], [mylora.LoraInjectedLinear], verbose=True\n",
    ")\n",
    "mylora.freeze_module(model)\n",
    "model.get_parameter('attn.QKV.lora_down.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylora.save_lora(model, 'tmp/lora1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injected lora (1x2x1) in attn.QKV\n",
      "Injected lora (1x2x1) in attn.C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7614],\n",
       "        [ 0.1908]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylib.set_all_seeds(1)\n",
    "model = A()\n",
    "mylora.inject_lora(\n",
    "    model, 2, 0, [\"Attention\"], [nn.Linear], [mylora.LoraInjectedLinear], verbose=True\n",
    ")\n",
    "mylora.freeze_module(model)\n",
    "model.get_parameter('attn.QKV.lora_down.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2017],\n",
       "        [0.4190]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylora.load_lora(model, 'tmp/lora1.pt')\n",
    "model.get_parameter('attn.QKV.lora_down.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layers:                12\n",
      "trainable layers:             0\n",
      "frozen layers:               12\n",
      "\n",
      "total params:                16\n",
      "trainable params:             0\n",
      "frozen params:               16\n"
     ]
    }
   ],
   "source": [
    "mylora.model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layers:                12\n",
      "trainable layers:             4\n",
      "frozen layers:                8\n",
      "\n",
      "total params:                16\n",
      "trainable params:             8\n",
      "frozen params:                8\n"
     ]
    }
   ],
   "source": [
    "mylora.unfreeze_lora(model)\n",
    "mylora.model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylora.set_scale(model, 0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_submodule('attn.QKV').scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
