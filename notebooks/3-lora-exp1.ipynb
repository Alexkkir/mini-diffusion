{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akkirr/IT/annotated-diffusion\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nan_to_num\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(linear_beta_schedule, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw) # PIL image of shape HWC\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 128\n",
    "# transform_aux = get_transform_aux(image_size)\n",
    "\n",
    "# x_start = transform_aux(image).unsqueeze(0)\n",
    "# x_start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor([40])\n",
    "\n",
    "# sampler.get_noisy_image(x_start, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot([sampler.get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520e8e9d1a724bd19a9beaed27cc82e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ab52c2fabf4c7e90bf5531a8161ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de61355e36d46018651700eaf1f7800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fashion_mnist (/Users/akkirr/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6388bb48e9604504a0618d3ebc7f8d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from the hub\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "image_size = 28\n",
    "channels = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image transformations (e.g. using torchvision)\n",
    "transform = Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "# define function\n",
    "def transforms(examples):\n",
    "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
    "   del examples[\"image\"]\n",
    "\n",
    "   return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pixel_values'])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = Path(\"./results\")\n",
    "results_folder.mkdir(exist_ok = True)\n",
    "save_and_sample_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].shape[0]\n",
    "      batch = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "      t = torch.randint(0, sampler.timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "      loss = sampler.p_losses(model, batch, t, loss_type=\"huber\")\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # save generated images\n",
    "      if step != 0 and step % save_and_sample_every == 0:\n",
    "        milestone = step // save_and_sample_every\n",
    "        batches = num_to_groups(4, batch_size)\n",
    "        all_images_list = list(map(lambda n: sampler.sample(model, batch_size=n, channels=channels), batches))\n",
    "        all_images = torch.cat(all_images_list, dim=0)\n",
    "        all_images = (all_images + 1) * 0.5\n",
    "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.named_children())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.QKV = nn.Linear(1, 1)\n",
    "        self.C = nn.Linear(1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_proj = nn.Linear(1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "class LoraInjected(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.src_linear = nn.Linear(1, 1)\n",
    "        self.A = nn.Linear(1, 1)\n",
    "        self.B = nn.Linear(1, 1)\n",
    "        self.dropout = nn.Dropout1d()\n",
    "\n",
    "class A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.just_linear = nn.Linear(1, 1)\n",
    "        self.attn = Attention()\n",
    "        self.time_embedder = TimeEmbedding()\n",
    "\n",
    "a = A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QKV', 'C']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x[1] for x in find_modules(a, [\"Attention\"], (nn.Linear,), (LoraInjected,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unet\n",
      "init_conv Conv2d\n",
      "time_mlp Sequential\n",
      "time_mlp.0 SinusoidalPositionEmbeddings\n",
      "time_mlp.1 Linear\n",
      "time_mlp.2 GELU\n",
      "time_mlp.3 Linear\n",
      "downs ModuleList\n",
      "downs.0 ModuleList\n",
      "downs.0.0 ResnetBlock\n",
      "downs.0.0.mlp Sequential\n",
      "downs.0.0.mlp.0 SiLU\n",
      "downs.0.0.mlp.1 Linear\n",
      "downs.0.0.block1 Block\n",
      "downs.0.0.block1.proj WeightStandardizedConv2d\n",
      "downs.0.0.block1.norm GroupNorm\n",
      "downs.0.0.block1.act SiLU\n",
      "downs.0.0.block2 Block\n",
      "downs.0.0.block2.proj WeightStandardizedConv2d\n",
      "downs.0.0.block2.norm GroupNorm\n",
      "downs.0.0.block2.act SiLU\n",
      "downs.0.0.res_conv Identity\n",
      "downs.0.1 ResnetBlock\n",
      "downs.0.1.mlp Sequential\n",
      "downs.0.1.mlp.0 SiLU\n",
      "downs.0.1.mlp.1 Linear\n",
      "downs.0.1.block1 Block\n",
      "downs.0.1.block1.proj WeightStandardizedConv2d\n",
      "downs.0.1.block1.norm GroupNorm\n",
      "downs.0.1.block1.act SiLU\n",
      "downs.0.1.block2 Block\n",
      "downs.0.1.block2.proj WeightStandardizedConv2d\n",
      "downs.0.1.block2.norm GroupNorm\n",
      "downs.0.1.block2.act SiLU\n",
      "downs.0.1.res_conv Identity\n",
      "downs.0.2 Residual\n",
      "downs.0.2.fn PreNorm\n",
      "downs.0.2.fn.fn LinearAttention\n",
      "downs.0.2.fn.fn.to_qkv Conv2d\n",
      "downs.0.2.fn.fn.to_out Sequential\n",
      "downs.0.2.fn.fn.to_out.0 Conv2d\n",
      "downs.0.2.fn.fn.to_out.1 GroupNorm\n",
      "downs.0.2.fn.norm GroupNorm\n",
      "downs.0.3 Sequential\n",
      "downs.0.3.0 Rearrange\n",
      "downs.0.3.1 Conv2d\n",
      "downs.1 ModuleList\n",
      "downs.1.0 ResnetBlock\n",
      "downs.1.0.mlp Sequential\n",
      "downs.1.0.mlp.0 SiLU\n",
      "downs.1.0.mlp.1 Linear\n",
      "downs.1.0.block1 Block\n",
      "downs.1.0.block1.proj WeightStandardizedConv2d\n",
      "downs.1.0.block1.norm GroupNorm\n",
      "downs.1.0.block1.act SiLU\n",
      "downs.1.0.block2 Block\n",
      "downs.1.0.block2.proj WeightStandardizedConv2d\n",
      "downs.1.0.block2.norm GroupNorm\n",
      "downs.1.0.block2.act SiLU\n",
      "downs.1.0.res_conv Identity\n",
      "downs.1.1 ResnetBlock\n",
      "downs.1.1.mlp Sequential\n",
      "downs.1.1.mlp.0 SiLU\n",
      "downs.1.1.mlp.1 Linear\n",
      "downs.1.1.block1 Block\n",
      "downs.1.1.block1.proj WeightStandardizedConv2d\n",
      "downs.1.1.block1.norm GroupNorm\n",
      "downs.1.1.block1.act SiLU\n",
      "downs.1.1.block2 Block\n",
      "downs.1.1.block2.proj WeightStandardizedConv2d\n",
      "downs.1.1.block2.norm GroupNorm\n",
      "downs.1.1.block2.act SiLU\n",
      "downs.1.1.res_conv Identity\n",
      "downs.1.2 Residual\n",
      "downs.1.2.fn PreNorm\n",
      "downs.1.2.fn.fn LinearAttention\n",
      "downs.1.2.fn.fn.to_qkv Conv2d\n",
      "downs.1.2.fn.fn.to_out Sequential\n",
      "downs.1.2.fn.fn.to_out.0 Conv2d\n",
      "downs.1.2.fn.fn.to_out.1 GroupNorm\n",
      "downs.1.2.fn.norm GroupNorm\n",
      "downs.1.3 Sequential\n",
      "downs.1.3.0 Rearrange\n",
      "downs.1.3.1 Conv2d\n",
      "downs.2 ModuleList\n",
      "downs.2.0 ResnetBlock\n",
      "downs.2.0.mlp Sequential\n",
      "downs.2.0.mlp.0 SiLU\n",
      "downs.2.0.mlp.1 Linear\n",
      "downs.2.0.block1 Block\n",
      "downs.2.0.block1.proj WeightStandardizedConv2d\n",
      "downs.2.0.block1.norm GroupNorm\n",
      "downs.2.0.block1.act SiLU\n",
      "downs.2.0.block2 Block\n",
      "downs.2.0.block2.proj WeightStandardizedConv2d\n",
      "downs.2.0.block2.norm GroupNorm\n",
      "downs.2.0.block2.act SiLU\n",
      "downs.2.0.res_conv Identity\n",
      "downs.2.1 ResnetBlock\n",
      "downs.2.1.mlp Sequential\n",
      "downs.2.1.mlp.0 SiLU\n",
      "downs.2.1.mlp.1 Linear\n",
      "downs.2.1.block1 Block\n",
      "downs.2.1.block1.proj WeightStandardizedConv2d\n",
      "downs.2.1.block1.norm GroupNorm\n",
      "downs.2.1.block1.act SiLU\n",
      "downs.2.1.block2 Block\n",
      "downs.2.1.block2.proj WeightStandardizedConv2d\n",
      "downs.2.1.block2.norm GroupNorm\n",
      "downs.2.1.block2.act SiLU\n",
      "downs.2.1.res_conv Identity\n",
      "downs.2.2 Residual\n",
      "downs.2.2.fn PreNorm\n",
      "downs.2.2.fn.fn LinearAttention\n",
      "downs.2.2.fn.fn.to_qkv Conv2d\n",
      "downs.2.2.fn.fn.to_out Sequential\n",
      "downs.2.2.fn.fn.to_out.0 Conv2d\n",
      "downs.2.2.fn.fn.to_out.1 GroupNorm\n",
      "downs.2.2.fn.norm GroupNorm\n",
      "downs.2.3 Conv2d\n",
      "ups ModuleList\n",
      "ups.0 ModuleList\n",
      "ups.0.0 ResnetBlock\n",
      "ups.0.0.mlp Sequential\n",
      "ups.0.0.mlp.0 SiLU\n",
      "ups.0.0.mlp.1 Linear\n",
      "ups.0.0.block1 Block\n",
      "ups.0.0.block1.proj WeightStandardizedConv2d\n",
      "ups.0.0.block1.norm GroupNorm\n",
      "ups.0.0.block1.act SiLU\n",
      "ups.0.0.block2 Block\n",
      "ups.0.0.block2.proj WeightStandardizedConv2d\n",
      "ups.0.0.block2.norm GroupNorm\n",
      "ups.0.0.block2.act SiLU\n",
      "ups.0.0.res_conv Conv2d\n",
      "ups.0.1 ResnetBlock\n",
      "ups.0.1.mlp Sequential\n",
      "ups.0.1.mlp.0 SiLU\n",
      "ups.0.1.mlp.1 Linear\n",
      "ups.0.1.block1 Block\n",
      "ups.0.1.block1.proj WeightStandardizedConv2d\n",
      "ups.0.1.block1.norm GroupNorm\n",
      "ups.0.1.block1.act SiLU\n",
      "ups.0.1.block2 Block\n",
      "ups.0.1.block2.proj WeightStandardizedConv2d\n",
      "ups.0.1.block2.norm GroupNorm\n",
      "ups.0.1.block2.act SiLU\n",
      "ups.0.1.res_conv Conv2d\n",
      "ups.0.2 Residual\n",
      "ups.0.2.fn PreNorm\n",
      "ups.0.2.fn.fn LinearAttention\n",
      "ups.0.2.fn.fn.to_qkv Conv2d\n",
      "ups.0.2.fn.fn.to_out Sequential\n",
      "ups.0.2.fn.fn.to_out.0 Conv2d\n",
      "ups.0.2.fn.fn.to_out.1 GroupNorm\n",
      "ups.0.2.fn.norm GroupNorm\n",
      "ups.0.3 Sequential\n",
      "ups.0.3.0 Upsample\n",
      "ups.0.3.1 Conv2d\n",
      "ups.1 ModuleList\n",
      "ups.1.0 ResnetBlock\n",
      "ups.1.0.mlp Sequential\n",
      "ups.1.0.mlp.0 SiLU\n",
      "ups.1.0.mlp.1 Linear\n",
      "ups.1.0.block1 Block\n",
      "ups.1.0.block1.proj WeightStandardizedConv2d\n",
      "ups.1.0.block1.norm GroupNorm\n",
      "ups.1.0.block1.act SiLU\n",
      "ups.1.0.block2 Block\n",
      "ups.1.0.block2.proj WeightStandardizedConv2d\n",
      "ups.1.0.block2.norm GroupNorm\n",
      "ups.1.0.block2.act SiLU\n",
      "ups.1.0.res_conv Conv2d\n",
      "ups.1.1 ResnetBlock\n",
      "ups.1.1.mlp Sequential\n",
      "ups.1.1.mlp.0 SiLU\n",
      "ups.1.1.mlp.1 Linear\n",
      "ups.1.1.block1 Block\n",
      "ups.1.1.block1.proj WeightStandardizedConv2d\n",
      "ups.1.1.block1.norm GroupNorm\n",
      "ups.1.1.block1.act SiLU\n",
      "ups.1.1.block2 Block\n",
      "ups.1.1.block2.proj WeightStandardizedConv2d\n",
      "ups.1.1.block2.norm GroupNorm\n",
      "ups.1.1.block2.act SiLU\n",
      "ups.1.1.res_conv Conv2d\n",
      "ups.1.2 Residual\n",
      "ups.1.2.fn PreNorm\n",
      "ups.1.2.fn.fn LinearAttention\n",
      "ups.1.2.fn.fn.to_qkv Conv2d\n",
      "ups.1.2.fn.fn.to_out Sequential\n",
      "ups.1.2.fn.fn.to_out.0 Conv2d\n",
      "ups.1.2.fn.fn.to_out.1 GroupNorm\n",
      "ups.1.2.fn.norm GroupNorm\n",
      "ups.1.3 Sequential\n",
      "ups.1.3.0 Upsample\n",
      "ups.1.3.1 Conv2d\n",
      "ups.2 ModuleList\n",
      "ups.2.0 ResnetBlock\n",
      "ups.2.0.mlp Sequential\n",
      "ups.2.0.mlp.0 SiLU\n",
      "ups.2.0.mlp.1 Linear\n",
      "ups.2.0.block1 Block\n",
      "ups.2.0.block1.proj WeightStandardizedConv2d\n",
      "ups.2.0.block1.norm GroupNorm\n",
      "ups.2.0.block1.act SiLU\n",
      "ups.2.0.block2 Block\n",
      "ups.2.0.block2.proj WeightStandardizedConv2d\n",
      "ups.2.0.block2.norm GroupNorm\n",
      "ups.2.0.block2.act SiLU\n",
      "ups.2.0.res_conv Conv2d\n",
      "ups.2.1 ResnetBlock\n",
      "ups.2.1.mlp Sequential\n",
      "ups.2.1.mlp.0 SiLU\n",
      "ups.2.1.mlp.1 Linear\n",
      "ups.2.1.block1 Block\n",
      "ups.2.1.block1.proj WeightStandardizedConv2d\n",
      "ups.2.1.block1.norm GroupNorm\n",
      "ups.2.1.block1.act SiLU\n",
      "ups.2.1.block2 Block\n",
      "ups.2.1.block2.proj WeightStandardizedConv2d\n",
      "ups.2.1.block2.norm GroupNorm\n",
      "ups.2.1.block2.act SiLU\n",
      "ups.2.1.res_conv Conv2d\n",
      "ups.2.2 Residual\n",
      "ups.2.2.fn PreNorm\n",
      "ups.2.2.fn.fn LinearAttention\n",
      "ups.2.2.fn.fn.to_qkv Conv2d\n",
      "ups.2.2.fn.fn.to_out Sequential\n",
      "ups.2.2.fn.fn.to_out.0 Conv2d\n",
      "ups.2.2.fn.fn.to_out.1 GroupNorm\n",
      "ups.2.2.fn.norm GroupNorm\n",
      "ups.2.3 Conv2d\n",
      "mid_block1 ResnetBlock\n",
      "mid_block1.mlp Sequential\n",
      "mid_block1.mlp.0 SiLU\n",
      "mid_block1.mlp.1 Linear\n",
      "mid_block1.block1 Block\n",
      "mid_block1.block1.proj WeightStandardizedConv2d\n",
      "mid_block1.block1.norm GroupNorm\n",
      "mid_block1.block1.act SiLU\n",
      "mid_block1.block2 Block\n",
      "mid_block1.block2.proj WeightStandardizedConv2d\n",
      "mid_block1.block2.norm GroupNorm\n",
      "mid_block1.block2.act SiLU\n",
      "mid_block1.res_conv Identity\n",
      "mid_attn Residual\n",
      "mid_attn.fn PreNorm\n",
      "mid_attn.fn.fn Attention\n",
      "mid_attn.fn.fn.to_qkv Conv2d\n",
      "mid_attn.fn.fn.to_out Conv2d\n",
      "mid_attn.fn.norm GroupNorm\n",
      "mid_block2 ResnetBlock\n",
      "mid_block2.mlp Sequential\n",
      "mid_block2.mlp.0 SiLU\n",
      "mid_block2.mlp.1 Linear\n",
      "mid_block2.block1 Block\n",
      "mid_block2.block1.proj WeightStandardizedConv2d\n",
      "mid_block2.block1.norm GroupNorm\n",
      "mid_block2.block1.act SiLU\n",
      "mid_block2.block2 Block\n",
      "mid_block2.block2.proj WeightStandardizedConv2d\n",
      "mid_block2.block2.norm GroupNorm\n",
      "mid_block2.block2.act SiLU\n",
      "mid_block2.res_conv Identity\n",
      "final_res_block ResnetBlock\n",
      "final_res_block.mlp Sequential\n",
      "final_res_block.mlp.0 SiLU\n",
      "final_res_block.mlp.1 Linear\n",
      "final_res_block.block1 Block\n",
      "final_res_block.block1.proj WeightStandardizedConv2d\n",
      "final_res_block.block1.norm GroupNorm\n",
      "final_res_block.block1.act SiLU\n",
      "final_res_block.block2 Block\n",
      "final_res_block.block2.proj WeightStandardizedConv2d\n",
      "final_res_block.block2.norm GroupNorm\n",
      "final_res_block.block2.act SiLU\n",
      "final_res_block.res_conv Conv2d\n",
      "final_conv Conv2d\n"
     ]
    }
   ],
   "source": [
    "for name, mod in model.named_modules():\n",
    "    print(name, mod.__class__.__name__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
